{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_model = GPT4All(r\"/home/pslearner/LLM_models/mistral-7b-openorca.gguf2.Q4_0-004.gguf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Prompt engineering RAG with LLM's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided prompt engineering for retrieval agumented generation (RAGs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sixty_inch_qled_quant = 15\n",
    "seventy_two_inch_qled_quant = 4\n",
    "ninty_inch_qled_quant = 0\n",
    "basic_microwave_quant = 9\n",
    "card_quant = 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_system_prompt = f\"\"\"\n",
    "You are a helpful AI assistant who will answer questions about the selection of products below. If asked questions about products\n",
    "that are not listed and described below reply with 'I am unable to help with that, would you like to connect with a representative?' \n",
    "Truthfully reply as if you have no access to personal information or a database to be referenced.\n",
    "\n",
    "Current products in stock are as follows:\n",
    "60 inch flat screen QLED TV with {sixty_inch_qled_quant} currently in stock priced at $250,\n",
    "72 inch flat screen QLED TV with {seventy_two_inch_qled_quant} currently in stock priced at $350,\n",
    "90 inch flat screen QLED TV with {ninty_inch_qled_quant} currently in stock priced at $450,\n",
    "standard microwave with {basic_microwave_quant} currently in stock priced at $150 while on sale down from 200,\n",
    "get well soon cards with {card_quant} currently in stock priced at $7.25,\n",
    "\n",
    "QLED stands for Quantum Dot Light Emitting Diode, which is a type of liquid crystal display (LCD) that uses nanocrystals to produce different\n",
    "colors of light. QLED TVs have a layer of quantum dots between the backlight and the pixels on the screen, with each dot size corresponding\n",
    "to a different color. This results in more saturated colors and higher peak brightness than traditional LED TVs. QLED TVs are also well-suited\n",
    "for brightly-lit environments and have a long lifespan.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mistral_model.chat_session(system_prompt=rag_system_prompt):\n",
    "    response = mistral_model.generate(prompt= \"\", max_tokens=50, temp=0.1, streaming=False)\n",
    "    for x in mistral_model.current_chat_session:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mistral_model.chat_session(system_prompt=rag_system_prompt):\n",
    "    response = mistral_model.generate(prompt= \"\", max_tokens=50, temp=0.1, streaming=False)\n",
    "    response = mistral_model.generate(prompt= \"\", max_tokens=50, temp=0.1, streaming=False)\n",
    "    response = mistral_model.generate(prompt= \"\", max_tokens=50, temp=0.1, streaming=False)\n",
    "\n",
    "    for x in mistral_model.current_chat_session:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mistral_model.chat_session(system_prompt=rag_system_prompt):\n",
    "    response = mistral_model.generate(prompt= \"\", max_tokens=50, temp=0.1, streaming=False)\n",
    "    response = mistral_model.generate(prompt= \"\", max_tokens=50, temp=0.1, streaming=False)\n",
    "    response = mistral_model.generate(prompt= \"\", max_tokens=50, temp=0.1, streaming=False)\n",
    "    response = mistral_model.generate(prompt= \"\", max_tokens=50, temp=0.1, streaming=False)\n",
    "\n",
    "    for x in mistral_model.current_chat_session:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mistral_model.chat_session(system_prompt=rag_system_prompt):\n",
    "    response = mistral_model.generate(prompt= \"\", max_tokens=50, temp=0.1, streaming=False)\n",
    "    for x in mistral_model.current_chat_session:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Ensuring proper responses for security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mistral_model.chat_session(system_prompt=rag_system_prompt):\n",
    "    response = mistral_model.generate(prompt= \"\", max_tokens=50, temp=0.1, streaming=False)\n",
    "\n",
    "    for x in mistral_model.current_chat_session:\n",
    "        print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
